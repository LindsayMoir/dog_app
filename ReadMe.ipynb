{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Scientist Nanodegree: Capstone (What Dog Do You Look Like?)\n",
    "\n",
    "\n",
    "![image_head](images/toller.png)\n",
    "\n",
    "The purpose of this project is to use a convolutional neural network (CNN) to predict dog breeds. The final pipeline code evaluates if an image is a dog or a human; if a dog, predicts a breed; and if a human face tells what dog breed the person most resembles. \n",
    "\n",
    "How I proceeded exactly and what results I achieved can be read in my blog post: [What Dog Do You Look Like?](https://medium.com/@tragoes/38cb55cce966)\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Hardware and Software Requirements](#hard_soft_requirements)\n",
    "3. [Folder](#folders)\n",
    "4. [Getting Started](#getting_started)\n",
    "5. [Running dog_app_pipeline](#run_app_pipeline)\n",
    "    1. [Detect Humans Face](#detect_humans_face)\n",
    "    1. [Get InceptionV3 Model Trained](#run_dog_app)\n",
    "    2. [Classify Dog Breeds](#classify_dog_breeds)\n",
    "6. [Results](#results)    \n",
    "7. [Author](#author)\n",
    "8. [Project Motivation](#motivation)\n",
    "9. [Acknowledgements](#acknowledgement)\n",
    "\n",
    "\n",
    "<a name=\"introduction\"></a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is the Udacity Capstone Project for the Data Science NanoDegree (DSND). The challenge is to identify whether an image is a dog or a human. If a dog, to classify the breed. If a human, to suggest what breed looks most similar to the person's face. It is often said that dogs look like their owners! For this project I did the following.\n",
    "\n",
    "+ Did Exploratory Data Analysis (EDA) on 2 dog breed datasets. One from Udacity (originally from ImageNet) and the other is the Stanford University dog dataset. \n",
    "+ Produced 5 final models after dozens of rejected models in two different notebooks.\n",
    "+ Produced another notebook called dog_app_pipeline.ipynb. This is all of the code to run the solution to this project using the best model (InceptionV3 with 3 new layers). \n",
    "\n",
    "\n",
    "<a name=\"hard_soft_requirements\"></a>\n",
    "\n",
    "## Hardware and Software Requirements\n",
    "\n",
    "I ran this on an i7 32 GB DDR4-SDRAM 1128 GB HDD + 1 TB SSD with a NVIDIA® GeForce® GTX 960M GPU. The OS is Windows 10. The project would have been extremely difficult witout the GPU. Tensorflow 2.4 was installed.\n",
    "\n",
    "Required libraries:\n",
    "\n",
    "+ Python 3.8\n",
    "+ Keras\n",
    "+ Matplotlib\n",
    "+ Numpy\n",
    "+ OpenCV\n",
    "+ Pandas\n",
    "+ PIL\n",
    "+ Scikit-Learn\n",
    "+ TensorFlow\n",
    "\n",
    "The import libraries were:\n",
    "```\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "from extract_bottleneck_features import *\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import random\n",
    "from shutil import copyfile, copytree, rmtree\n",
    "from sklearn.datasets import load_files \n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from tensorflow import config\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50           \n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import load_model, Model, Sequential\n",
    "from tensorflow.keras.preprocessing import image   \n",
    "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n",
    "```\n",
    "\n",
    "\n",
    "<a name=\"folders\"></a>\n",
    "\n",
    "## Folders\n",
    "\n",
    "You should use this folder structure to run this code. My folder structure on my machine is slightly different. I have 2 drives. One is a SSD and the other is an old style hard drive. The hard drive is being backed up by OneDrive. I keep large files on the C drive and they do NOT get backed up. The files for this project are several GB.\n",
    "```\n",
    "D:\n",
    "│   dog_app.ipynb\n",
    "│   dog_app_pipeline.ipynb\n",
    "│   extract_bottleneck_features.py\n",
    "│   iv3_stanford.ipynb\n",
    "│   README.md\n",
    "│   requirements.txt\n",
    "│\n",
    "├───bottleneck_features\n",
    "├───data\n",
    "│   ├───dog_images\n",
    "│   └───lfw\n",
    "├───haarcascades\n",
    "│       haarcascade_frontalface_alt.xml\n",
    "│\n",
    "├───images\n",
    "│       VARIOUS\n",
    "│\n",
    "└───saved_models\n",
    "│       VARIOUS\n",
    "│\n",
    "├───stanford\n",
    "│   ├───train\n",
    "│   └───test\n",
    "│   └───valid\n",
    "```\n",
    "\n",
    "\n",
    "<a name=\"getting_started\"></a>\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. Python 3.8 or above is required.\n",
    "2. Clone this repository and go to the project's root directory.\n",
    "3. Download the [dog dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip). Unzip the folder and put the 3 files (test, train and valid) in the repository in the folder ```data/dog_images```. If one of these folders does not exist, create it. \n",
    "4. Download the [human dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip). Unzip the folder and put it in the repository in folder ```data/lfw```. If one of these folders does not exist, create it. \n",
    "5. Download the [stanford dog dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/). Unzip the folder and put it in the repository in the folder ```stanford```. \n",
    "6. Download the [VGG-16](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG16Data.npz), [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz), and the [InceptionV3](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features and put them in the repository in folder ```bottleneck_features```. If this folder does not exist, please create it. \n",
    "7. Start and run the notebook ```dog_app.ipynb```.\n",
    "8. Start and run the notebook ```dog_app_pipeline.ipynb```. This notebook is the uncluttered version that produces the solution from the already trained InceptionV3 model. You get that model trained in dog_app.ipynb.\n",
    "9. Start the notebook ```iv3_stanford.ipynb```. This notebook runs only the stanford dataset and only one model.\n",
    "\n",
    "\n",
    "\n",
    "<a name=\"run_app_pipeline\"></a>\n",
    "\n",
    "## Running dog_app_pipeline.ipynb\n",
    "\n",
    "  \n",
    "<a name=\"detect_humans_face\"></a>\n",
    "\n",
    "### Detect People's Faces\n",
    "\n",
    "I used OpenCV's implementation of [Haar feature-based cascade classifiers](https://docs.opencv.org/master/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.\n",
    "\n",
    "![detect_face](images/detect_face.png)\n",
    "\n",
    "\n",
    "<a name=\"run_dog_app\"></a>\n",
    "\n",
    "### Get InceptionV3 Model Trained\n",
    "\n",
    "The pipeline requires the trained InceptionV3 model. You will need to run the dog_app.ipynb notebook first to get it.\n",
    "<a name=\"classify_dog_breeds\"></a>\n",
    "\n",
    "\n",
    "### Classify Dog Breeds\n",
    "\n",
    "I used transfer learning with InceptionV3 as the final chosen model for the pipeline. I used this to determine the breed of dog from dog pictures. If a dog picture is supplied, then it provides the breed. If a human image is provided, it chooses a breed that it thinks looks the most like the person's face! If neither a human or a dog image is provided, the code produces 2 images as follows. \"Not a Dog / Human\" for the picture and a question mark image with the caption \"Only Pictures of Dogs / Humans!\n",
    "\n",
    "![predictions](images/predictions.png)\n",
    "\n",
    "\n",
    "<a name=\"results\"></a>\n",
    "\n",
    "## Results\n",
    "\n",
    "![results](images/results.png)\n",
    "\n",
    "### VGG16_two_off_the_bottom\n",
    "\n",
    "We used the [VGG16 model](https://neurohive.io/en/popular-networks/vgg16/) for Transfer Learning, removed the bottom 2 layers and replaced them with 4 new layers. We do not use the [VGG19 model](https://iq.opengenus.org/vgg19-architecture/) in this notebook. It is MUCH larger than VGGG16. \n",
    "\n",
    "```\n",
    "top_vgg16 = base_vgg16.output\n",
    "top_vgg16 = Flatten(name=\"flatten\")(top_vgg16)\n",
    "top_vgg16 = Dense(1024, activation='relu')(top_vgg16)\n",
    "top_vgg16 = Dropout(0.2)(top_vgg16)\n",
    "output_layer = Dense(nof_classes, activation='softmax')(top_vgg16)\n",
    "\n",
    "Total params: 40,542,149\n",
    "Trainable params: 28,187,269\n",
    "Non-trainable params: 12,354,880\n",
    "```\n",
    "The results were the worst of all the CNNs. Accuracy was 1.1%, training time was 42 minutes and it took 1.5 minutes to do a really bad prediction on 6 images. \n",
    "\n",
    "\n",
    "### My_CNN_Model\n",
    "\n",
    "This is a from scratch CNN (hand coded NOT Transfer Learning). It trained to ~ 9% acccuracy after 20 epochs.\n",
    "```\n",
    "Total params: 6,428,709\n",
    "Trainable params: 6,428,709\n",
    "Non-trainable params: 0\n",
    "```\n",
    "This was actually quite good, considering the amount of effort required to produce this result. There are 133 breeds in this dataset!\n",
    "\n",
    "\n",
    "### IV3_model_16_layers\n",
    "\n",
    "This uses the InceptionV3 model [InceptionV3 model](https://arxiv.org/pdf/1512.00567v3.pdf). This model is in a separate notebook called iv3_model_16_layers. We provided 3 new layers\n",
    "```\n",
    "top_model = model.output\n",
    "top_model = GlobalAveragePooling2D(input_shape=(5, 5, 2048))(top_model)\n",
    "top_model = Dropout(0.5)(top_model)\n",
    "output_layer = Dense(n_classes, activation='softmax')(top_model)\n",
    "```\n",
    "and also retrained 16 of the last layers from the original inceptionV3 model.\n",
    "```\n",
    "for layer in model.layers[:-16]:\n",
    "  layer.trainable = False\n",
    "```\n",
    "It had an accuracy of 71% and took 54 minutes to train. \n",
    "```\n",
    "Total params: 22,048,664\n",
    "Trainable params: 641,144\n",
    "Non-trainable params: 21,407,520\n",
    "```\n",
    "This is running with the Stanford training and test data. There are 120 classes as opposed to 133 classes in the Udacity dataset. It is also using data augmentation via generators.\n",
    "\n",
    "\n",
    "### VGG16_model\n",
    "\n",
    "This model trained extremely rapidly. \n",
    "```\n",
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "```\n",
    "It took only about 8 seconds to train AND it had an accuracy of ~ 75%!\n",
    "```\n",
    "Total params: 68,229\n",
    "Trainable params: 68,229\n",
    "Non-trainable params: 0\n",
    "```\n",
    "\n",
    "### IV3_model\n",
    "\n",
    "The clear winner was the InceptionV3 model.  \n",
    "```\n",
    "IV3_model = Sequential()\n",
    "IV3_model.add(GlobalAveragePooling2D(input_shape=train_IV3.shape[1:]))\n",
    "IV3_model.add(Dropout(0.5))\n",
    "IV3_model.add(Dense(133, activation='softmax'))\n",
    "```\n",
    "It only took 8 seconds to train on 5 epochs and it reached ~ 85% accuracy which is outstanding.\n",
    "\n",
    "![accuracy](images/accuracy.png)\n",
    "\n",
    "```\n",
    "Total params: 272,517\n",
    "Trainable params: 272,517\n",
    "Non-trainable params: 0\n",
    "```\n",
    "We used this model for the dog_app_pipeline.ipynb \n",
    "\n",
    "\n",
    "### Summary\n",
    "I am impressed with Transfer Learning. You get high accuracy, fast training times, and tiny coding complexity. It is not often that you get way more for less time, money, and effort!\n",
    "\n",
    "\n",
    "<a name=\"author\"></a>\n",
    "\n",
    "## Author\n",
    "\n",
    "+ [Lindsay Moir, Website](http://lindsaymoir.com/) and [Lindsay Moir, GitHub](https://github.com/LindsayMoir/)\n",
    "\n",
    "\n",
    "<a name=\"motivation\"></a>\n",
    "\n",
    "## Project Motivation: \n",
    "\n",
    "I started my journey in Data Science when my phone could understand me about 5 years ago. This was a stunning example of Natural Language Processing (NLP) which is a branch of Deep Learning (neural nets). CNN's are an example of neural nets being applied to computer vision. As a data scientist, CNNs (especially with Transer Learning) are a required addition to your knowledge arsenal.\n",
    "\n",
    "\n",
    "<a name=\"acknowledgement\"></a>\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I thank [Udacity](https://www.udacity.com/) for providing this opporunity to learn these technologies in a simple, convenient, and cost effective manner! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
